{"cells":[{"cell_type":"markdown","metadata":{"id":"VJEZ8P61An7I"},"source":["# Лабораторная работа 2\n","\n","## Линейная регрессия"]},{"cell_type":"markdown","source":["## Часть 1: Приближаем функции\n"],"metadata":{"id":"5kOKUEqSSU5j"}},{"cell_type":"markdown","metadata":{"id":"y3OmO16EAn7M"},"source":["### Метод наименьших квадратов: постановка задачи\n","\n","Рассмотрим систему уравнений $Xa = y$, в которой $a$ — столбец неизвестных. Её можно переписать в векторном виде\n","$$x_1 a_1 + x_2 a_2 + \\ldots + x_k a_k = y,$$\n","где $x_1,\\ldots,x_k$ — столбцы матрицы $X$. Таким образом, решить исходную систему означает найти линейную комбинацию векторов $x_1,\\ldots,x_k$, равную правой части. Но что делать, если такой линейной комбинации не существует? Геометрически это означает, что вектор $y$ не лежит в подпространстве $U = \\langle x_1,\\ldots, x_k\\rangle$. В этом случае мы можем найти *псевдорешение*: вектор коэффициентов $\\hat{a}$, для которого линейная комбинация $x_1 \\hat{a}_1 + x_2 \\hat{a}_2 + \\ldots + x_k \\hat{a}_k$ хоть и не равна в точности $y$, но является наилучшим приближением — то есть ближайшей к $y$ точкой $\\hat{y}$ подпространства $U$ (иными словами, ортогональной проекцией $y$ на это подпростанство). Итак, цель наших исканий можно сформулировать двумя эквивалентными способами:\n","\n","1. Найти вектор $\\hat{a}$, для которого длина разности $|X\\hat{a} - y|$ минимальна;\n","2. Найти ортогональную проекцию $\\hat{y}$ вектора $y$ на подпространство $U$ и представить её в виде $X\\hat{a}$.\n","\n","Далее мы будем предполагать, что векторы $x_1,\\ldots,x_k$ линейно независимы (если нет, то сначала имеет смысл выделить максимальную линейно независимую подсистему).\n","\n","На лекциях было показано, что проекция вектора $y$ на подпространство $U = \\langle x_1,\\ldots, x_k\\rangle$, записывается в виде\n","$$\\hat{y} = X\\left(X^TX\\right)^{-1}X^Ty,$$\n","и, соответственно, искомый вектор $\\hat{a}$ равен\n","$$\\hat{a} = \\left(X^TX\\right)^{-1}X^Ty.$$"]},{"cell_type":"markdown","metadata":{"id":"Wbh4XI_BAn7P"},"source":["### Задача линейной регрессии\n","\n","Начнём с примера. Допустим, вы хотите найти зависимость среднего балла S студента ФКН от его роста H, веса W, длины волос L и N — количества часов, которые он ежедневно посвящает учёбе. Представьте, что мы измерили все эти параметры для $n$ студентов и получили наборы значений: $S_1,\\ldots, S_n$, $H_1,\\ldots, H_n$ и так далее.\n","\n","Теперь мы хотим построить **модель**, т.е. определить алгоритм, который будет принимать на вход некоторый набор параметров (в данном случае это измеренные нами $H, W, L$ и $N$), и выдавать значение некоторой **целевой переменной** (в данном примере это $S$). Тут можно подбирать много разных умных моделей, но начать имеет смысл с самой простой, линейной:\n","\n","$$S = a_0 + a_1H + a_2W + a_3L + a_4N.$$\n","\n","Конечно, строгой линейной зависимости нет (иначе можно было бы радостно упразднить экзамены), но мы можем попробовать подобрать коэффициенты $a_0, a_1, a_2, a_3, a_4$, удовлетворяющие вот такому требованию:\n","$$\\sum_{i=1}^n\\left(S_i - ( a_0 + a_1H_i + a_2W_i + a_3L_i + a_4N_i)\\right)^2 \\longrightarrow \\min$$\n","Т.е. мы хотим, чтобы квадрат отклонения правой части от левой был поменьше для всех $n$ студентов. Введём несколько обозначений:\n","$$X =\n","\\begin{pmatrix}\n","1 & H_1 & W_1 & L_1 & N_1\\\\\n","1 & H_2 & W_2 & L_2 & N_2\\\\\n","\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n","1 & H_n & W_n & L_n & N_n\n","\\end{pmatrix}, \\qquad a=\n","\\begin{pmatrix}\n","a_0\\\\ a_1\\\\ \\vdots\\\\ a_4\n","\\end{pmatrix},\\qquad y=\n","\\begin{pmatrix}\n","S_1\\\\ S_2\\\\ \\vdots \\\\ S_n\n","\\end{pmatrix}.$$\n","\n","Теперь наше требование выше можно переписать в таком виде:\n","$$\n","\\sum_{i=1}^n\\left(S_i - ( a_0 + a_1H_i + a_2W_i + a_3L_i + a_4N_i)\\right)^2 = |y - Xa|^2 \\longrightarrow \\min\n","$$\n","\n","И теперь видно, что мы получили задачу на метод наименьших квадратов!\n","Решая эту задачу с помощью уже известных формул, получаем оценки коэффициентов $\\hat{a}_i$ ($i = 1\\ldots,5$)."]},{"cell_type":"markdown","metadata":{"id":"v76yazhJAn7R"},"source":["Теперь проговорим общую постановку задачи линейной регрессии. У нас есть $k$ переменных $x_1,\\ldots,x_k$ (\"регрессоров\"), через которые мы хотим выразить \"объясняемую переменную\" $y$:\n","$$y = a_1x_1 + a_2x_2 + \\ldots + a_kx_k$$\n","Значения всех переменных мы измерили $n$ раз (у $n$ различных объектов,  в $n$ различных моментов времени - это зависит от задачи). Подставим эти данные в предыдущее равенство:\n","$$\\begin{pmatrix}\n","y_1\\\\ y_2 \\\\ \\vdots \\\\ y_n\n","\\end{pmatrix} = \n","a_1\\begin{pmatrix}\n","x_{11} \\\\ x_{21} \\\\ \\vdots \\\\ x_{n1} \\end{pmatrix} + a_2\\begin{pmatrix}\n","x_{12} \\\\ x_{22} \\\\ \\vdots \\\\ x_{n2} \\end{pmatrix} + \\ldots + a_k\\begin{pmatrix}\n","x_{1k} \\\\ x_{2k} \\\\ \\vdots \\\\ x_{nk} \\end{pmatrix}$$\n","(здесь $x_{ij}$ - это значение $j$-го признака на $i$-м измерении). Это удобно переписать в матричном виде:\n","$$\\begin{pmatrix}\n","x_{11} & x_{12} & \\ldots & x_{1k}\\\\\n","x_{21} & x_{22} & \\ldots & x_{2k}\\\\\n","\\dots & \\dots & \\dots & \\dots\\\\\n","x_{n1} & x_{n2} & \\ldots & x_{nk}\n","\\end{pmatrix} \\cdot\n","\\begin{pmatrix}\n","a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_k\n","\\end{pmatrix} = \n","\\begin{pmatrix}\n","y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n","\\end{pmatrix}$$\n","или коротко $Xa = y$. Поскольку на практике эта система уравнений зачастую не имеет решения (ибо зависимости в жизни редко бывают действительно линейными), методом наименьших квадратов ищется псевдорешение."]},{"cell_type":"markdown","metadata":{"id":"kDX5t_BxAn7T"},"source":["### Оценка качества. Обучение и тест \n","\n","После того, как вы построили регрессию и получили какую-то зависимость объясняемой переменной от регрессоров, настаёт время оценить качество регрессии. Есть много разных функционалов качества; мы пока будем говорить только о самом простом и очевидном из них: о среднеквадратичной ошибке (mean square error). Она равна\n","$$\\frac1{n}|X\\hat{a} - y|^2 = \\frac1{n}\\sum_{i=1}^n\\left(\\hat{a}_1x_{i1} + \\hat{a}_2x_{i2} + \\ldots + \\hat{a}_kx_{ik} - y_i\\right)^2$$\n","\n","В целом, хочется искать модели с наименьшей mean square error на имеющихся данных. Однако слишком фанатичная гонка за минимизацией ошибки может привести к печальным последствиям, в чём Вам предстоит убедиться в ходе выполнения этой лабораторной.\n","\n","Чтобы не попадать в эту ловушку, данные обычно делят на обучающие (по которым строят модель и оценивают коэффициенты) и тестовые. Лучшей стоит счесть ту модель, для которой значение функционала качества будет меньше."]},{"cell_type":"markdown","metadata":{"id":"CMrD0HArAn7T"},"source":["### Правила оформления графиков\n","При работе с данными часто неудобно делать какие-то выводы, если смотреть на таблицу и числа в частности, поэтому важно уметь визуализировать данные. \n","\n","У matplotlib, конечно же, есть [документация](https://matplotlib.org/users/index.html) с большим количеством [примеров](https://matplotlib.org/examples/), но для начала достаточно знать про несколько основных типов графиков:\n","- plot — обычный поточечный график, которым можно изображать кривые или отдельные точки;\n","- hist — гистограмма, показывающая распределение некоторой величины;\n","- scatter — график, показывающий взаимосвязь двух величин;\n","- bar — столбцовый график, показывающий взаимосвязь количественной величины от категориальной.\n","\n","Ещё одна билиотека для визуализации: [seaborn](https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html). Это надстройка над matplotlib, иногда удобнее и красивее делать визуализации через неё. \n","\n","При выполнении этой лабораторной Вы столкнётесь с необходимостью рисовать большое количество графиков. Не забывайте про базовые принципы построения приличных графиков:\n","- оси должны быть подписаны, причём не слишком мелко;\n","- у графика должно быть название;\n","- если изображено несколько графиков, то необходима поясняющая легенда;\n","- для точек из разных выборок необходимо использовать разные цвета;\n","- все линии на графиках должны быть чётко видны (нет похожих цветов или цветов, сливающихся с фоном);\n","- если отображена величина, имеющая очевидный диапазон значений (например, проценты могут быть от 0 до 100), то желательно масштабировать ось на весь диапазон значений (исключением является случай, когда вам необходимо показать малое отличие, которое незаметно в таких масштабах).\n","\n","Помните, что проверяющий имеет право снизить оценку за неопрятные графики."]},{"cell_type":"markdown","metadata":{"id":"3Ivj-lm0An7U"},"source":["### Формат сдачи\n","Задания сдаются через систему Anytask. Инвайт можно найти на странице курса. Присылать необходимо ноутбук с выполненным заданием. Сам ноутбук называйте в формате homework-practice-02-linregr-Username.ipynb, где Username — Ваша фамилия."]},{"cell_type":"markdown","metadata":{"id":"sgYTlfJlAn7W"},"source":["### Задание 1. Метод наименьших квадратов (3.1 баллов)\n","\n","**ВАЖНО! В этом задании вам нельзя использовать циклы (в том числе рекурсии, генераторы и конструкции вида map/reduce), кроме цикла в задании 1.5, который мы вам оставили. За наличие цикла в вашем коде мы поставим вам не более половины от стоимости задания.**"]},{"cell_type":"markdown","metadata":{"id":"h_HVV2__An7W"},"source":["Скачайте файлы ``train.txt`` и ``test.txt``. Каждый из файлов содержит два столбца чисел, разделённых пробелами: в первом — некоторое число точек (значения аргумента $x$), во втором — значения некоторой функции $y = f(x)$ в этих точках, искажённые случайным шумом. Функцию $f$ мы вам не скажем. Ваша задача — по данным из файла ``train.txt`` (будем называть их обучающей выборкой) подобрать функцию $y = g(x)$, пристойно приближающую неизвестную вам зависимость."]},{"cell_type":"markdown","metadata":{"id":"qhI6CxL0An7Y"},"source":["Загрузим обучающие и тестовые данные (из файла ``test.txt``)."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ca7tXdbiAn7Z"},"outputs":[],"source":["import numpy as np\n","from os.path import join\n","\n","PATH = \"./\" ### Вставить заглушку (не забудьте ввести правильный путь!)\n","\n","data_train = np.loadtxt(join(PATH, \"train.txt\"), delimiter=',')\n","data_test = np.loadtxt(join(PATH, \"test.txt\"), delimiter=',')"]},{"cell_type":"markdown","metadata":{"id":"k3GshOvnAn7b"},"source":["**0. [0 баллов]** Разделим значения $x$ и $y$"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"1BIIDVGjAn7b"},"outputs":[],"source":["X_train = data_train[:,0]\n","y_train = data_train[:,1]\n","\n","# Сделайте то же для тестовой выборки\n","#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","X_test = data_test[:,0]\n","y_test = data_test[:,1]"]},{"cell_type":"markdown","metadata":{"id":"PRLl0nkWAn7c"},"source":["**1. [0,15 балла]** Найдите с помощью метода наименьших квадратов линейную функцию $y = kx + b$, наилучшим образом приближающую неизвестную зависимость. Полезные функции: ``numpy.ones(n)`` для создания массива из единиц длины $n$ и ``numpy.concatenate((А, В), axis=1)`` для слияния двух матриц по столбцам (пара ``А`` и ``В`` превращается в матрицу ``[A B]``). Напечатайте этот многочлен в виде $kx+b$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTMuTy7IAn7d"},"outputs":[],"source":["#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"]},{"cell_type":"markdown","metadata":{"id":"Bnburua6An7e"},"source":["**2. [0,15 балла]** Нарисуйте на плоскости точки $(x_i, y_i)$ из обеих выборок и полученную линейную функцию."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Um_XtCIRAn7e"},"outputs":[],"source":["#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"]},{"cell_type":"markdown","metadata":{"id":"crzpARViAn7f"},"source":["**3. [0,4 балла]** Глядя на данные, подумайте, многочленом какой степени можно было бы лучше всего приблизить эту функцию с точки зрения минимизации среднеквадратичной ошибки на обучающей выборке. Найдите этот многочлен и сохраните его коэффициенты в массив `poly_coef` (от младшего члена к старшему). Обязательно обоснуйте выбор степени многочлена."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSywLPifAn7g"},"outputs":[],"source":["#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"]},{"cell_type":"markdown","metadata":{"id":"bVlu8GtRAn7h"},"source":["**4. [0,15 балла]** Нарисуйте его график на одном чертеже вместе с точками $(x_i, y_i)$ из обеих выборок. Удалось ли графику пройти через все точки из выборки? Попробуйте объяснить, почему?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2EjDMWVQAn7i"},"outputs":[],"source":["#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"]},{"cell_type":"markdown","metadata":{"id":"mlVLDqILAn7j"},"source":["**5. [0,5 балла]** Для $k = 1,2,3,\\ldots,10$ найдите многочлен $\\hat{f}_k$ степени $k$, наилучшим образом приближающий неизвестную зависимость. Сохраните найденные коэффициенты многочленов в матрицу `poly_coefs` (коэффициенты многочлена степени $j$ должны быть сохранены в $j$-й строке матрицы от младшего члена к старшему)."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"_kz4i82kAn7j"},"outputs":[],"source":["for k in range(10):\n","  #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","\n","# Простая проверка, что матрица poly_coefs имеет столько строк, сколько нужно\n","assert poly_coefs.shape[0] == 10"]},{"cell_type":"markdown","metadata":{"id":"XZm2UpUEAn7k"},"source":["**6. [0,6 балл]** Для каждого из многочленов из задания 1.5, а также для многочлена из задания 1.4 найдите среднеквадратическую ошибку на обучающих данных и на тестовых данных: $\\frac1{n}\\sum_{i=1}^n\\left( \\hat{f}_k(x_i) - y_i \\right)^2$. Полученные значения ошибок сохраните в матрицу `errors` высоты 11 и ширины 2. В первом столбце матрицы должны стоять значения ошибок на тренировочных данных, во втором --- на тестовых. В первых 10 строках матрицы должны стоять значения ошибок для многочленов соответствующих степений из задания 1.5, в 11-й строке --- значение ошибки для многочлена из задания 1.4."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byyH4arFAn7k"},"outputs":[],"source":["#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"]},{"cell_type":"markdown","metadata":{"id":"VKns2ndWAn7l"},"source":["**7. [0,5 балла]** Для $k = 1,2,3,4,6$ нарисуйте графики полученных многочленов на одном чертеже вместе с точками $(x_i, y_i)$ из обеих выборок (возможно, график стоит сделать побольше; это делается командой `plt.figure(figsize=(width, height))`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vwt574k1An7m"},"outputs":[],"source":["#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"]},{"cell_type":"markdown","metadata":{"id":"YXMClHOOAn7n"},"source":["**8. [0,65 балла]** Что происходит с ошибкой на тестовых данных при росте степени многочлена? Казалось бы, чем больше степень, тем более сложным будет многочлен и тем лучше он будет приближать нашу функцию. Подтверждают ли это ваши наблюдения? Как вам кажется, чем объясняется поведение ошибки на тестовых данных при $k = 10$? Как называется наблюдаемый вами эффект?"]},{"cell_type":"markdown","metadata":{"id":"-6rGUQMqAn7n"},"source":["**Ответ:** ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"]},{"cell_type":"markdown","source":["## Часть 2: Обучаем машины"],"metadata":{"id":"iwhYglpqSgh1"}},{"cell_type":"markdown","metadata":{"id":"KFN99QhUAn7o"},"source":["### Задание 2. Линейная регрессия (3 балла)"]},{"cell_type":"markdown","source":["В этом задании вам предстоит заняться предсказанием цен на алмазы. Скачайте файл `diamonds_features.csv`, содержащий данные, с которыми вам предстоит работать. Запустите следующую ячейку:"],"metadata":{"id":"EIPGe5d0JSgq"}},{"cell_type":"code","source":["data = np.loadtxt(\"diamonds_features.csv\", delimiter=\",\", dtype=\"str\")[1:, :]\n","print(data[:5, :])"],"metadata":{"id":"8_G9QdO6PdP4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Мы вывели для вас первые 5 строчек данных. Описания значений в колонках вы можете найти в файле `diamonds_descriptions.txt`.\n","\n","Глобально в этом задании вам предстоит научиться строить модель линейной регрессии для предсказания цены алмаза по его признакам. Данные устроены таким образом, что в каждой строчке располагаются признаки, описывающие объект (алмаз), а в каждой фиксированной колонке --- всевозможные значения конкретного признака для всех объектов в данных."],"metadata":{"id":"ghn2xvL5P8BK"}},{"cell_type":"markdown","metadata":{"id":"fJEFOEpJAn7p"},"source":["**0. [0 баллов]** Разделите выборку на обучающую и тестовую. Делать это лучше случайным образом (ведь вы не знаете, как создатели датасета упорядочили объекты); рекомендуем вам для этого функцию [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) с параметром `test_size=0.3`. Обязательно зафиксируйте параметр `random_state`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RoFzvGRBAn7q"},"outputs":[],"source":["#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"]},{"cell_type":"markdown","source":["Теперь у вас есть две выборки, каждая из которых содержит случайное подмножество наших данных. Обучающую выборку мы в дальнейшем будем использовать для обучения модели линейной регрессии, а тестовую, соответственно, для теста.\n","\n","**ВАЖНО: за обучение модели на тестовой выборке будем ставить 0 за весь пункт!**"],"metadata":{"id":"dVO-0MsaTXXT"}},{"cell_type":"markdown","source":["### Работаем с числовыми данными"],"metadata":{"id":"JEQLiBg-_zfK"}},{"cell_type":"markdown","source":["Сейчас ваши данные содержат как привычные вам числовые признаки, описывающие объект, так и категориальные признаки, которые описывают принадлежность объекта к той или иной категории, и в данных представляются строками. Что делать с данными, которые представлены строками --- мы разберёмся чуть позже, а пока давайте забудем, что они у нас есть, и будем работать только с числовыми."],"metadata":{"id":"bHSfXGDeC0b1"}},{"cell_type":"markdown","source":["**0,5. [0 баллов]** Заполните переменные `numeric_features` и `categorical_features` индексами столбцов в данных, содержащих числовые и категориальные признаки соответственно. После этого можно проверить, что массив `data[:, numeric_features]` содержит только числовые данные."],"metadata":{"id":"KGGUh0UBEHSG"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","numeric_features = []\n","categorical_features = []\n","print(data[:, numeric_features])"],"metadata":{"id":"kZWCocxzE-dv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**1. [0,3 баллов]** Решите задачу линейной регрессии для предсказания цены алмаза по его числовым признакам (не забудьте, что линейная модель должна также содержать свободный от признаков член) и протестируйте ваше решение на тестовых данных. Выведите полученные регрессионные коэффициенты для каждого признака. Попробуйте проинтерпретировать полученные коэффициенты: сравните их друг с другом, посмотрите на их знаки и на абсолютную величину и попробуйте на основе этого придать им житейский смысл. Согласуется ли построенная интерпретация модели с вашими представлениями о жизни? \n","\n","*Примечание: обратите внимание, сейчас все данные у вас хранятся в строковом виде. Чтобы работать с ними как с числами, необоходимо явно привести их к типу float. В NumPy для этого есть специальная удобная команда astype.*"],"metadata":{"id":"Fiqc6nBwFVpP"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"Q-rpxCQLH1VY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Интерпретация модели здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"Se87BnStIAOK"}},{"cell_type":"markdown","source":["Если вы всё сделали правильно, у вас должны были получиться довольно большие по абсолютной величине коэффициенты."],"metadata":{"id":"6uGLZR-0TxaP"}},{"cell_type":"markdown","source":["**2. [0,15 баллов]** Попробуйте объяснить, почему ситуация, когда модель имеет большие по модулю коэффиценты, нежелательна?\n","\n","Подсказка: Вы можете попытаться проиллюстрировать свои рассуждения, искусственно описав с помощью исследуемых признаков два алмаза, которые по вашему мнению должны иметь одинаковую цену, и проверить, выдаст ли построенная модель близкие предсказания на этих объектах."],"metadata":{"id":"58iMlV0TUHHz"}},{"cell_type":"markdown","source":["Ваше обоснование здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・"],"metadata":{"id":"a3oKgoUjWiem"}},{"cell_type":"markdown","source":["**3. [0.2 баллов]** Нарисуйте гистограмму, характеризующую распределение объясняемой переменной в обучающей выборке. На такой диаграмме по оси `x` должны быть сгруппированные значения объясняемой переменной, а по оси `y` --- количество объектов выборки с таким значением. Столбцов на вашем графике должно быть не менее 50."],"metadata":{"id":"ycsEn6t9nwLB"}},{"cell_type":"code","source":["#╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"itrJbGIl-I9Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["В левой части гистограммы располагаются небольшие значения целевой переменной, и объектов с таким значением в выборке довольно много (не забываем, что у нас тут не абстрактная математика в вакууме, мы тут цену алмазов предсказываем!); с увеличением значения таргета (целевой переменной) количество соответствующих объектов в выборке убывает. Сначала число объектов убывает довольно стремительно, но с некоторого момента разница в высоте между соседними столбцами уже почти незаметна. Такое явление называется \"распределение с тяжёлым хвостом\", и оно не очень желательно при построении модели."],"metadata":{"id":"SEvOIN0DQyhZ"}},{"cell_type":"markdown","source":["Попробуем избавиться от тяжёлого хвоста. Для этого нужно применить к таргету какую-нибудь медленнорастущую биективную функцию, например логарифм. \n","\n","<!-- Однако вспомним, что мы намеренно отмасштабировали таргет таким образом, чтобы среди его значений был 0, поэтому нужно придумать ещё какое-то преобразование, чтобы все значения целевой переменной попали в область определения логарифма. -->"],"metadata":{"id":"PO37Qg7BTyqw"}},{"cell_type":"markdown","source":["**4. [0,25 баллов]** Прологарифмируйте целевую переменную и нарисуйте график распределения логарифмированного таргета. Сильно ли он отличается от предыдущего графика? Попробуйте объяснить, почему явление \"распределения с тяжёлым хвостом\" нежелательно при построении модели."],"metadata":{"id":"nXb63LraZaCL"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"AdgFVEebZwPT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ваше обоснование здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・"],"metadata":{"id":"jrlDuorpEiU8"}},{"cell_type":"markdown","source":["Снова обучите линейную модель, считая целевой переменной логарифмированный таргет. Помните, что чтобы протестировать полученную линейную модель, важно взять экспоненту от полученных предсказаний."],"metadata":{"id":"Ty0T3kk5EiZ-"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"NbWAprptKEt0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Удалось ли добиться лучшего качества?"],"metadata":{"id":"NTSiJAeYE4EM"}},{"cell_type":"markdown","source":["Ответ: ╰( ͡° ͜ʖ ͡° )つ──☆*:・"],"metadata":{"id":"zugn9a3fE8nb"}},{"cell_type":"markdown","source":["### Работаем с категориальными признаками"],"metadata":{"id":"NHn1U5xJaPVH"}},{"cell_type":"markdown","source":["До сих пор мы работали с данными, которые представляются числами. Однако часто в датасетах объекты описываются не только признаками, содержащими числа, но и строковыми признаками, определяющими принадлежность объекта к некоторой категории. Такие признаки называются категориальными. Важно не выкидывать их из модели, потому что часто они содержат довольно полезную информацию об объекте, которую хорошая модель должна обязательно учитывать.\n","\n","В этом разделе мы будем рассматривать только категориальные признаки, временно забыв про числовые.\n","\n","Естесственно, нам нужно придумать, как закодировать категориальные признаки числами. Есть довольно много подходов к такой кодировке, но мы рассмотрим два самых базовых. Первая мысль, которая приходит в голову такая: давайте занумеруем все категории фиксированного признака, и заменим их строковое представление соответствующим численным."],"metadata":{"id":"_CLrKbkJdK2b"}},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Zh1vygTdAn7u"},"source":["**5. [0,3 баллов]** Реализуйте описанную выше процедуру кодирования для всех категориальных признаков в выборке. Вы можете сделать это самостоятельно, а можете разобраться в классе `OrdinalEncoder` библиотеки `sklearn`. Обучите линейную модель на закодированных категориальных признаках и протестируйте. "]},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"FETTkQQGo_-P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Подумайте, в чём потенциальные проблемы такого кодирования? (Подсказка: пусть изначально мы закодировали категорию A числом 1, а категорию B числом 2, а потом решили сделать наоборот: A <-> 2, B <-> 1. Подумайте, может ли измениться качество модели? А должно ли?)"],"metadata":{"id":"9O9CmAifpAJK"}},{"cell_type":"markdown","source":["Ваше обоснование здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n"],"metadata":{"id":"ZBkiDk5qqtSk"}},{"cell_type":"markdown","source":["Теперь давайте попробуем исправить недочёты такого способа кодирования категориальных признаков, придумав другой метод. Пусть некоторый признак имеет 4 различные категории: {A, B, C, D}. Заменим этот признак на 4 бинарных признака. Каждый из новых признаков будет являться индикатором одной из этих четырёх категорий: если некоторый объект имел категорию A, то после нашего кодирования значение признака, соответствующего категории А у этого объекта будет равно $1$, а значения оставшихся трёх признаков будут $0$. Небольшая иллюстрация: пусть так выглядела выборка до кодирования\n","\n","| Объект | Признак |\n","| :----------:|:-:|\n","| X | \"B\" |\n","| Y | \"D\" |\n","\n","А так будет выглядеть после кодирования:\n","\n","| Объект | Признак == \"A\" | Признак == \"B\" | Признак == \"C\" | Признак == \"D\" |\n","| :----------:|:-:|:-:|:-:|:-:|\n","| X | 0 | 1 | 0 | 0 |\n","| Y | 0 | 0 | 0 | 1 |\n"],"metadata":{"id":"3NY2scS1rAuU"}},{"cell_type":"markdown","source":["**6. [0,3 баллов]** Реализуйте описанную выше процедуру кодирования для всех категориальных признаков в выборке. Вы можете сделать это самостоятельно, а можете разобраться в классе `OneHotEncoder` библиотеки `sklearn`. Обучите линейную модель на закодированных категориальных признаках и протестируйте. "],"metadata":{"id":"U_VHgVKjtpwr"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"2Rrs69UxtyvR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Подумайте, в чём потенциальные проблемы такого кодирования? Какие недочёты прошлого метода кодирования исправляет предложенный способ? Какие недочёты есть у этого метода кодирования, которых нет у предыдущего? Который из двух методов выдаёт лучшее качество?"],"metadata":{"id":"rSOmE1iLty7n"}},{"cell_type":"markdown","source":["Ваше обоснование здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"8grwr0ZBuIgq"}},{"cell_type":"markdown","source":["**7. [0,5 баллов]** Обучите линейную модель на всех данных, выполнив все необходимые преобразования. Постарайтесь добиться меньшей ошибки модели, чем во всех предыдущих заданиях."],"metadata":{"id":"IBEMSIfuzoGJ"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"HiXPa1UcQYje"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Feature engineering"],"metadata":{"id":"TOt1HIMMg0nu"}},{"cell_type":"markdown","source":["#### Описание задания.\n","\n","В рамках этого задания вам предстоит улучшить модель и сделать предсказания на выборке из файла`diamonds_test.csv`, ответов к которой мы вам не дадим. Полученные предсказания вам нужно будет сдать в соревнование на сайте [kaggle.com](https://www.kaggle.com/c/hse-lab-linear-regression-21-22/overview). Подробнее о том, как сдавать предсказания, написано в конце раздела. Разумеется, чтобы иметь возможность сдать предсказания, вам нужно быть зарегистрированными на сайте. \n","\n","Оценка за задание рассчитывается исходя из ошибки полученных вами предсказаний и состоит из базовой части и бонусных баллов. Базовая часть оценки определяется следующими пороговыми значениями качества:\n","\n","*   MSE $\\leq 1950000$ ---- 0,25 балла\n","*   MSE $\\leq 1370000$ ---- 0,5 балла\n","*   MSE $\\leq 1280000$ ---- 0,75 балла\n","*   MSE $\\leq 1170000$ ---- 1 балл\n","\n","Если ваша модель выдаёт качество сильно лучше, чем требуется в пороге на 1 балл, то вы можете претендовать на бонусные баллы, приняв участие в полноценном соревновании, смысл которого в том, чтобы получить качество как можно лучше (добиться наименьшей возможной ошибки). \n","\n","Если вы хотите поучаствовать, то просто продолжайте улучшать вашу модель и сдавать предсказания в соревнование. Бонусные баллы будут рассчитываться на основе ошибки полученных вами предсказаний и вашей позиции в лидерборде соревнования. Как именно будет рассчитываться оценка --- мы вам сообщим после завершения соревнования. Точно можем сказать, что наибольшее число баллов за задание получит только один студент --- тот, кто будет находиться на первой позиции в лидерборде на момент дедлайна. Человек на следующем месте уже получит немного меньше, следующий --- ещё меньше, и так далее. Каждый студент, качество предсказаний которого хоть немного лучше нашего порога на 1 балл, обязательно получит бонусные баллы.\n","\n","Чтобы подтвердить своё участие в соревновании, вам нужно будет сдать вашу модель вместе с лабораторной (лучше в отдельном файле, но можно её и прямо здесь написать).\n","\n","**Максимум бонусных баллов за задание: 3**"],"metadata":{"id":"Ua8Hc_I2NdE8"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"esBQkVIY25zH"}},{"cell_type":"markdown","source":["В оставшейся части данного раздела приводятся несколько заданий, за выполнение котороых вы не получите баллов. Эти задания даны специально, чтобы вы посмотрели, какие есть идеи для дальнейшего улучшения модели.  Разумеется, мы не требуем от вас, чтобы ваша финальная модель следовала указаниям из этих заданий --- вы можете вообще не выполнять ни одного задания отсюда, оцениваться будут только сданные вами предсказания.\n","\n","\n","\n"],"metadata":{"id":"ujFRVw0_focC"}},{"cell_type":"markdown","source":["До сих пор мы только исследовали вопрос, как подготовить признаки из датасета, чтобы модель могла на их основе делать какие-то предсказания. При этом до сих пор мы никак не учитывали специфичность задачи: действительно, до этого момента нас интересовало только то, является ли признак числовым или категориальным, мы никак не учитывали физический смысл признаков. Однако часто датасеты предоставляют лишь самую общую информацию об объектах: признаки, которые легко измерить и запомнить. В таком случае при анализе данных нужно выдумывать собственные признаки, которые некоторым образом зависят от уже представленных в датасете признаков и **ни в коем случае не зависят от объясняемой переменной**.\n","\n","В общем случае процесс придумывания новых признаков довольно творческий. Есть, конечно, некоторые более-менее общие подходы, позволяющие улучшить качество модели путём добавления новых признаков, но часто аналитики придумывают признаки, основываясь не только на известных эвристиках, но и на собственном представлении о жизни.\n","\n","Давайте теперь поисследуем наши данные и подумаем, какие признаки можно добавить, чтобы улучшить модель."],"metadata":{"id":"Qa714bBHg8_B"}},{"cell_type":"markdown","source":["**8.** Нарисуйте графики, отображающие зависимость цены алмаза от веса, длины, ширины и глубины (всего должно получиться 4 графика). Проинтерпретируйте наблюдаемую на них зависимость. Согласуется ли она с вашими представлениями о жизни?\n","\n","*Примечание: для изображения зависимости одной величины от другой лучше всего подходит график типа scatter.*"],"metadata":{"id":"bPhiy1IhiLow"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"nQzcpqqNkGYq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Подумайте, какую функцию можно применить к признакам, чтобы зависимость целевой переменной от них больше напоминала линейную?"],"metadata":{"id":"jd6J9SQlkGwZ"}},{"cell_type":"markdown","source":["Ваше обоснование здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"hU63JZex0h_P"}},{"cell_type":"markdown","source":["**9.** На одном рисунке изобразите график зависимости целевой переменной от признака depth и график зависимости целевой переменной от признака table. Чтобы картинка получилась читаемой, сделайте графики полупрозрачными (параметр `alpha`). Что вы можете сказать о получившемся рисунке? Правда ли, что модели будет достаточно оставить только один из этих признаков? Убедитесь в этом, нарисовав аналогичные графики зависимости признаков depth и table от других числовых признаков.\n","\n","*Примечание: почитайте про plt.subplots, чтобы узнать, как красиво рисовать несколько графиков.*"],"metadata":{"id":"1L7iSqah0mYd"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"UqTp9xWY28Av"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Казалось бы, мы тут пытаемся придумать новые признаки, чтобы улучшить модель, зачем нам выбрасывать уже имеющиеся признаки? Дело в том, что если в выборке есть \"похожие\" в некотором смысле признаки, то модель, обученная на таких данных, может выдавать худший результат, чем если бы похожих признаков в выборке не было. Продемонстрируем это явление на практике."],"metadata":{"id":"6kZjZ2Ww3YF0"}},{"cell_type":"markdown","source":["**10.** Обучите линейную модель, предсказывающую цену алмазов на признаках x, y, z из наших данных. Замерьте качество полученной линейной модели на тестовой выборке, после чего добавьте признак $\\tau = \\frac{x}{2} + 4y - z + 1$, и снова обучите линейную модель. Которая из моделей добилась лучшего качества?"],"metadata":{"id":"5x3TzT7V4gDP"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"ktbNFzsz5gt9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Объясните, почему наличие похожих признаков в выборке заметно ухудшает качество, выдаваемое линейной моделью?"],"metadata":{"id":"RbZ4u40h_MyO"}},{"cell_type":"markdown","source":["Ваше обоснование здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"gEINdpQy_XcA"}},{"cell_type":"markdown","source":["Итак, мы обсудили, исходя из каких соображений можно выбирать преобразования над имеющимися признаками и когда для модели может быть полезнее выкинуть некоторый признак из выборки, чем оставить его. Теперь давайте придумаем совсем новый признак. Для этого снова посмотрим на такие характеристики алмазов, как длина, ширина и глубина. Эти характеристики несомненно важны для предсказания цены алмазов, и все построенные нами модели находили какую-то закономерность, но теперь давайте мы попробуем их объединить в новом признаке `объём`. Для начала будем считать, что объём алмаза неплохо приближается объёмом параллелепипеда с такими же измерениями."],"metadata":{"id":"argZX3YMCb6e"}},{"cell_type":"markdown","source":["**11.** Обучите линейную модель на 4 признаках: x, y, z, и объём параллелепипеда со сторонами (x, y, z). Сравните качество полученной модели с качеством, выдаваемым моделью, обученной на признаках x, y, z."],"metadata":{"id":"DtlMCLwEDhhG"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"5fgSn6IZELmT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Понятно, что алмазы редко выглядят как идеальные параллелепипеды (хотя, конечно, многое зависит от огранки), и чаще встречаются алмазы, форма которых скорее напоминает эллипсоид. Снова обучите линейную модель на 4 признаках: x, y, z и объём эллипсоида с осями (x, y, z). Сравните полученную модель с предыдущей. Сильно ли отличается качество у этих моделей? Объясните, почему так получилось?"],"metadata":{"id":"6XuJr72iGBSy"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"5ZzmZmNhIKDo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Если вам стало интересно, можете попробовать обучить модель, аппроксимируя объём алмаза объёмом тетраэдра. Впрочем, если вы ответили на предыдущий вопрос, вы уже наверняка представляете, что получится. В любом случае мы не настаиваем =)\n","\n","Следующий трюк, который мы обсудим, --- это бинаризация признаков. Суть заключается в том, что мы берём некоторый признак из выборки и делаем из него несколько бинарных признаков (т.е. таких признаков, значения которых могут быть либо $0$, либо $1$). Частный случай бинаризации признаков мы уже разбирали, когда занимались кодированием категориальных признаков: тогда мы брали признак $P$, описывающий принадлежность объекта к одной из $N$ категорий ${A_1, \\ldots, A_N}$, и делали из него $N$ признаков вида $[P = A_1], [P = A_2], \\ldots [P = A_N]$.\n","\n","Однако это не единственный способ бинаризовать признаки. Допустим $P$ --- числовой признак, тогда мы, например, можем выбрать $k$ порогов ${t_1, \\ldots, t_k}$ и превратить признак $P$ в $k$ бинарных признаков $[P \\leq t_1], [t_1 < P \\leq t_2], \\ldots, [t_{k-1} < P \\leq t_k]$.\n","\n","Если же у нас есть два признака $P$ и $F$, мы можем применять булевы операции к их бинаризациям, например $[P < t_1 \\vee F > h_1], [t_2 < P \\leq t_3 \\wedge h_2 \\leq F \\leq h_3], [(P = t_4 \\vee F = h_4) \\wedge P \\leq t_5]$, и т.д.\n","\n","Давайте с помощью бинаризации выделим объекты, цена которых скорее всего будет высокой, и объекты, цена которых скорее всего будет низкой. Делать мы это будем на основе категориальных признаков (закодированных любым способом, однако если вы предпочитаете кодирование бинаризацией, т.е. второй способ, то при выполнении задания не забывайте, что признаки должны быть линейно независимы). Чтобы вы понимали, как именно категориальные признаки характеризуют алмазы, мы приложили вам картинку `categorical_descriptions`.\n","\n","![](https://lh3.google.com/u/0/d/1xzqbnwyOazKNiiJxNo_AUh7iUsTONC-0=w1920-h1090-iv1)"],"metadata":{"id":"c7r5Je0uI6pj"}},{"cell_type":"markdown","source":["**12.** Обучите линейную модель, предсказывающую цену алмазов на категориальных признаках из наших данных. Замерьте качество полученной линейной модели на тестовой выборке, после чего добавьте признаки, которые явно выделят алмазы, цена на которые скорее всего будет высокой, и алмазы, цена которых скорее всего будет низкой, и снова обучите линейную модель. Вы можете также добавить и свои признаки и использовать не только категориальные признаки (например, если вам захочется добавить признак \"у алмаза лучшая чистота И алмаз совершенно бесцветный И объём алмаза не меньше $t$\"). Удалось ли добиться прироста в качестве?"],"metadata":{"id":"ADOr1ZH6Y1OQ"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"7sxhbqDPZyfH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Как сдавать предсказания?\n","\n","Передайте в написанную нами функцию `make_submission` ваши предсказания. Когда функция отработает, у вас в рабочей директории появится файл `submission.csv`, который вам и нужно сдать в соревнование."],"metadata":{"id":"AbIbDwNci6Mf"}},{"cell_type":"code","source":["def make_submission(y_pred):\n","  file = np.arange(1, len(y_pred) + 1)\n","  file = file.reshape(-1, 1)\n","  file = np.concatenate([file, y_pred.reshape(-1, 1).astype(float)], axis=1)\n","  with open(\"submission.csv\", \"w\") as f:\n","    f.write(\"id,price\\n\")\n","    np.savetxt(f, file, delimiter=\",\", fmt=\"%d,%10.5f\")\n","\n","# пример вызова функции:\n","make_submission(np.zeros(16182))"],"metadata":{"id":"m4gv1RKjjtxA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Задание 3. Регуляризация (3.9 баллов)\n","\n"],"metadata":{"id":"4aHgP-6qfl0D"}},{"cell_type":"markdown","source":["Конечно, никто не гарантирует, что объясняемая переменная зависит от остальных характеристик именно линейно, поэтому мы можем неслабо улучшить модель, добавляя в неё нелинейные признаки на основе уже имеющихся. Один из наиболее распространённых подходов заключается в добавлении в выборку полиномиальных признаков некоторой фиксированной степени на основе уже имеющихся. Т.е., скажем, у нас есть признаки {P, F, G} и наша задача состоит в том, чтобы добавить в выборку полиномиальные признаки степени $2$ на основе имеющихся. Тогда мы добавим в выборку признаки\n","\n","$$\n","P^2, F^2, G^2, PF, PG, FG.\n","$$\n","\n","Аналогично для большего числа признаков и больших степеней.\n","\n","**1. [0,4 баллов]** Реализуйте функцию `add_polynomial_features`, принимающую массив признаков, и степень, и возвращающую полиномиальные признаки заданной степени."],"metadata":{"id":"6RcCzJLjm8a6"}},{"cell_type":"code","source":["def add_polynomial_features(features, degree):\n","  \"\"\"\n","    features: массив признаков\n","    degree:   показатель степени\n","  \"\"\"\n","  # ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n","  raise NotImplementedError()\n","\n","# Пример: получаем полиномиальные признаки степени 2 на основе признаков x, y, z:\n","poly_features = add_polynomial_features(data[:, [6, 7, 8]].astype(np.float64).T, 2)\n","assert poly_features.shape[0] == 6 "],"metadata":{"id":"_lnytPeGnEFP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Обучите линейную модель, добавив в выборку полиномиальные признаки степени $2$ от признаков carat и table. Сравните качество полученной модели с качеством модели из пункта 7 предыдущего задания."],"metadata":{"id":"TqwGy6HEnX6W"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"MPI707WOn1op"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Итак, мы узнали, что в выборку можно добавлять полиномиальные признаки от уже имеющихся числовых признаков и, как правило, это приводит к существенному приросту качества. Возникает естественный вопрос: почему бы не свести задачу построения линейной модели к подбору степени полиномиальных признаков? Ведь наверняка можно найти такую степень, при которой модель будет выдавать наилучшее возможное качество? Давайте исследуем этот вопрос."],"metadata":{"id":"_ihqwU4rvAKw"}},{"cell_type":"markdown","source":["**2. [0,3 баллов]** В этом задании вам нужно обучить 6 линейных моделей на полиномиальных признаках степеней от 1 до 6 от уже имеющихся признаков (carat, x, y, z). Нарисуйте график зависимости качества модели на обучающей и тестовой выборке от степени полиномиальных признаков в логарифмической шкале. Прокомментируйте полученный график: ожидали ли вы такого эффекта? "],"metadata":{"id":"Ug11Yg52v5WU"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"JGvdh2Wmwwh_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Наблюдаемый вами эффект называется переобучением: ситуация, когда модель становится слишком зависимой от обучающей выборки. Действительно, в процессе обучения модели мы стараемся минимизировать ошибку именно на обучающей выборке, и совершенно неочевидно, почему обученная модель будет обобщаться на объекты, которых она раньше не видела.\n","\n","Когда мы начали добавлять в выборку полиномиальные признаки большой степени, модель стала настолько сложной, что обрела способность запомнить обучающую выборку вместо того, чтобы извлекать из неё какие-то закономерности. Как следствие, когда мы подали модели на вход выборку, отличную от обучающей, мы получили колоссальную ошибку.\n","\n","Ситуация, когда качество модели на обучающей выборке гораздо ниже качества на тестовой выборке, является самым явным симптомом переобучения, но не единственным. Часто в ситуации переобучения можно наблюдать необычайно большие по модулю коэффициенты модели.\n","\n","Есть много способов борьбы с этим злом. Один из них --- регуляризация. Сейчас мы рассмотрим одну из её разновидностей --- **L2-регуляризацию**. Идея в том, чтобы подправить матрицу $X^TX$, сделав её \"получше\". Например, это можно сделать, заменив её на $(X^TX + \\lambda E)$, где $\\lambda > 0$ --- некоторый скаляр. Пожертвовав точностью на обучающей выборке, мы тем не менее получаем численно более стабильное псевдорешение $a = (X^TX + \\lambda E)^{-1}X^Ty$ и снижаем эффект переобучения. *Гиперпараметр* $\\lambda$ нужно подбирать, и каких-то универсальных способов это делать нет, но зачастую можно его подобрать таким, чтобы ошибка на тестовой выборке падала.\n","\n","Однако есть некоторые правила, которых стоит придерживаться при подборе коэффициента регуляризации. Обычно в такой ситуации всю выборку делят на три части: обучающую, *валидационную* и тестовую. Сначала по валидационной подбирают значение гиперпараметра, потом по обучающей строят модель, а по тестовой оценивают её итоговое качество. Кроме того, подбирать $\\lambda$ нужно по логарифметической сетке, чтобы узнать оптимальный порядок величины."],"metadata":{"id":"PKtmVN-Dww4h"}},{"cell_type":"markdown","source":["**3. [0,1 балла]** Подумайте, почему не стоит подбирать коэффициент регуляризации по обучающей выборке? По тестовой выборке?"],"metadata":{"id":"KRr312xvz-Cx"}},{"cell_type":"markdown","source":["Ваше обоснование здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"PjYAT3i-0GGX"}},{"cell_type":"markdown","source":["**4. [0,3 балла]** Рассмотрим линейную можель с полиномиальными признаками степени $4$ от признаков (carat, x, y, z). Качество такой модели на обучающей выборке уже заметно лучше, чем качество на тестовой. Попробуем исправить эту ситуацию, применив $L2$-регуляризацию. Поделите вашу обучающую выборку на две части в соотношении $8:2$. Большую часть вы вновь объявите обучающей выборкой, а меньшую --- валидационной. Подберите гиперпараметр $\\lambda$ по логарифмической сетке таким образом, чтобы линейная модель с \"подправленной\" матрицей Грама $X^TX$ выдавала лучшее качество на валидационной выборке. Обучите линейную модель с подобранным параметром $\\lambda$ на обучающей выборке и протестируйте её на тестовых данных. Сделайте вывод: помогла ли нам регуляризация?\n","\n","*Примечание: np.logspace вам в помощь.*"],"metadata":{"id":"nQlJbhvH0SBD"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"g5rBhgI93MXd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**5. [0,5 балла]** Повторите первое задание, но вместо 6 обычных линейных моделей обучите 6 регуляризованных линейных моделей. Для каждой модели вам нужно будет подобрать свой гиперпараметр $\\lambda$. Сравните полученный график с графиком из первого задания: для всякой ли степени мы смогли улучшить ситуацию? Как выдумаете, почему?"],"metadata":{"id":"wFeo516Z4ckd"}},{"cell_type":"code","source":["# ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"fKPFiHrN4-xO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**6. [1,15 балла]** Пусть заданы $X\\in \\text{Mat}_{n \\times k}(\\mathbb{R})$, $\\lambda \\geqslant 0$, а также известно, что $\\text{rk}~X = k \\leqslant n$. Решите следующую задачу оптимизации:\n","$$|Xa - y|^2 + \\lambda|a|^2\\rightarrow\\min\\limits_{a \\in \\mathbb{R}^k}.$$\n","\n","Заметим, что первое слагаемое --- это значение mean-square error (с точностью до домножения на некоторую константу) модели с коэффициентами $a = (a_1, \\ldots, a_k)$. Типичной задачей линейной регрессии является минимизация первого слагаемого по всем векторам $a$. Однако в данном случае кроме минимизации среднеквадратичной ошибки мы также добавляем условие, что при этом и веса модели должны быть как можно меньше. Действительно, мы уже убедились, что чем сложнее модель, тем меньше будет ошибка на обучающей выборке (а именно это ошибку мы и минимизируем в процессе обучения) и тем вероятнее модель будет иметь большие по модулю коэффициенты, а значит, и большое значение $|a|$. Добавляя второе слагаемое в рассматриваемый выше функционал, мы как бы задаём ограничение на абсолютную величину коэффициентов модели. При этом параметр $\\lambda$ явно будет указывать, насколько сильно мы будем штрафовать модель за большие коэффициенты. Заметим, что если $\\lambda = 0$, то мы получаем классическую задачу линейной регресии без регуляризации."],"metadata":{"id":"IwzU99F75DPx"}},{"cell_type":"markdown","source":["Ваше решение здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"],"metadata":{"id":"m0l8CtbO7uxo"}},{"cell_type":"markdown","metadata":{"id":"GVmoHb9PAn75"},"source":["**7. [0,25 балла]** При построении линейной модели у вас, как правило, есть также и свободный член, не зависящий от признаков из выборки. Подумайте, почему регуляризовать свободный член — плохая идея?"]},{"cell_type":"markdown","metadata":{"id":"Pz7uZUdjAn76"},"source":["Ваше обоснование здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"]},{"cell_type":"markdown","metadata":{"id":"Gdffe8TCAn76"},"source":["**8. [0,5 балла]** Пусть теперь $\\text{rk}~X < k$. Всегда ли в этом случае существует решение? Если существует, то является ли оно единственным? Ответ обоснуйте."]},{"cell_type":"markdown","metadata":{"id":"P1h46LL8An76"},"source":["Ваше обоснование здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"]},{"cell_type":"code","source":[""],"metadata":{"id":"kgXIcdfoFWgn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7gvthEgAn77"},"source":["**9. [0,4 балла]** Покажите, что если решений бесконечно много, то среди них обязательно найдутся решения со сколь угодно большими по модулю компонентами вектора $a$."]},{"cell_type":"markdown","metadata":{"id":"gVPOHJuZAn77"},"source":["Ваше обоснование здесь: ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"]},{"cell_type":"markdown","metadata":{"id":"l6hnfV4vAn7-"},"source":["### Добавление. QR-разложение"]},{"cell_type":"markdown","metadata":{"id":"Al3Mv6w8An7-"},"source":["**QR-разложением** матрицы $A$ (не обязательно квадратной) мы будем называть её представление в виде $A = QR$, где $Q$ — матрица с ортонормированными столбцами, а $R$ - верхнетреугольная матрица.\n","\n","Смысл QR-разложения следующий. Пусть $a_1,\\ldots,a_m$ — столбцы матрицы $A$, $q_1,\\ldots,q_t$ — столбцы матрицы $Q$. Тогда $q_1,\\ldots,q_t$ — это ортонормированный базис в подпространстве, являющемся линейной оболочкой векторов $a_1,\\ldots,a_m$, а в матрице $R$ записаны коэффициенты, с помощью которых $a_i$ выражаются через $q_1,\\ldots,q_t$.\n","\n","Находить QR-разложение заданной матрицы можно разными способами. Мы познакомим Вас не с самым лучшим из них, но по крайней мере с наиболее простым концептуально. Заметим, что ортогональный базис линейной оболочки можно найти с помощью ортогонализации Грама-Шмидта. При этом коэффициенты из матрицы $R$ получаются в качестве побочного продукта этого процесса:\n","\n","```python\n","for j = 1...n:\n","    q_j = a_j\n","    for i = 1,...,j-1:\n","        r_ij = (q_i, a_j)\n","        q_j = q_j - r_ij * q_i\n","    r_jj = |q_j|\n","    if r_jj == 0: # a_j in <a_1,...,a_j-1>\n","        # What would you do in this case?..\n","    q_j = q_j / r_jj\n","```\n","\n","Для нахождения QR-разложения Вы можете использовать библиотечную функцию `scipy.linalg.qr`."]},{"cell_type":"markdown","metadata":{"id":"UB3uxms8An7_"},"source":["Поскольку лабораторная про линейную регрессию, не так-то просто замять вопрос о том, какое же отношение QR-разложение имеет к задаче регрессии. Упомянем одно из возможных применений.\n","\n","Допустим, мы нашли QR-разложение матрицы $X$, а именно: $X = QR$. Тогда\n","$$X^TX = (QR)^T(QR) = R^TQ^TQR = R^TR$$"]},{"cell_type":"markdown","metadata":{"id":"2XfyZf8zAn7_"},"source":["Поскольку в задаче регрессии матрица $X$ обычно полного ранга (то есть её столбцы линейно независимы), матрица $R$ будет квадратной. Благодаря этому нашу обычную формулу для набора регрессионных коэффициентов $\\hat{a}$ можно переписать в следующем виде:\n","\n","$$\\hat{a} = (X^TX)^{-1}X^Ty = (R^TR)^{-1}(QR)^Ty = R^{-1}(R^T)^{-1}R^TQ^Ty = R^{-1}Q^Ty$$\n","\n","Как видите, формула стала проще. Более того, зачастую обращение матрицы $R$ может быть численно более устойчиво, чем обращение матрицы $X^TX$."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"HSE Lab (linear regression) 21-22.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}